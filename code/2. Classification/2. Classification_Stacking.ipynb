{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7869bb79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa5fe8-b735-429a-9573-e1a5ae5621dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9695f5e-3ef0-4e5c-8845-3d2cf3a81e51",
   "metadata": {},
   "source": [
    "# 1. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e63bce4-67d5-4992-be7d-c8823c1cbae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 載入乳癌資料集\n",
    "breast_cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab00c82-2c27-44b9-a7b4-66c3c2eee1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"特徵數量：\", breast_cancer.data.shape[1])\n",
    "print(\"樣本數量：\", breast_cancer.data.shape[0])\n",
    "print(\"類別種類：\", breast_cancer.target_names)\n",
    "print(\"目標分佈：\", pd.Series(breast_cancer.target).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62e0b5a-df73-44fa-859d-aa139f204c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉換為 DataFrame\n",
    "df = pd.DataFrame(data=breast_cancer.data, columns=breast_cancer.feature_names)\n",
    "\n",
    "# 如果你也想把標籤（target）加入進來，可以這樣：\n",
    "df['target'] = breast_cancer.target\n",
    "\n",
    "# 顯示前幾筆資料確認\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908a759-ab77-49cb-9c61-4e5e02f2cc79",
   "metadata": {},
   "source": [
    "### 十個基本特徵（對應腫瘤影像的性狀）\n",
    "| 特徵名稱               | 說明                                | 意義                              |\n",
    "|:-----------------------|:----------------------------------|:----------------------------------|\n",
    "| **radius**             | 腫瘤半徑                           | 反映腫瘤大小，較大的半徑可能與惡性腫瘤相關。 |\n",
    "| **texture**            | 紋理（灰度變化)                     | 描述腫瘤表面的規則性或不規則性，紋理不均勻通常與惡性腫瘤相關。 |\n",
    "| **perimeter**          | 腫瘤周長                           | 反映腫瘤邊緣的長度，周長較大的腫瘤通常較大，可能是惡性的。 |\n",
    "| **area**               | 面積平均                            | 描述腫瘤的面積，較大的面積可能與惡性腫瘤相關。 |\n",
    "| **smoothness**         | 邊界平滑程度                        | 描述腫瘤邊界的規則性，較平滑的邊界通常是良性腫瘤的特徵。 |\n",
    "| **compactness**        | 緊密度 = 周長² / 面積 - 1            | 緊密度高通常與良性腫瘤相關，低緊密度則可能與惡性腫瘤相關。 |\n",
    "| **concavity**          | 凹度（輪廓的內凹程度）               | 较高的凹度通常表示不規則邊界，這是惡性腫瘤的特徵。 |\n",
    "| **concave points**     | 凹點數量                           | 较多的凹點數量顯示腫瘤邊界不規則，這通常與惡性腫瘤相關。 |\n",
    "| **symmetry**           | 對稱性                             | 對稱性良好的腫瘤通常是良性的，惡性腫瘤形狀往往不對稱。數值越大，表示腫瘤的對稱性越好。 |\n",
    "| **fractal dimension**  | 分形維度（邊界複雜度）               | 較高的分形維度表示腫瘤邊界複雜，這通常與惡性腫瘤相關。 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273616a4-5c15-4d76-a8aa-deefab90ed80",
   "metadata": {},
   "source": [
    "### 特徵類型說明\n",
    "30 個特徵來自 10 個基本特徵，每個基本特徵都有以下三種統計量：\n",
    "\n",
    "* mean：平均值，提供整體的概覽，反映樣本中大部分腫瘤的典型特徵。    \n",
    "  意義：平均值可以幫助描述樣本中大多數腫瘤的一般情況，它是了解腫瘤普遍特徵的重要指標。對於某些特徵，較高或較低的平均值可能顯示出腫瘤的潛在風險。\n",
    "  \n",
    "* se（standard error）：標準誤差，衡量特徵在樣本中的變異性，幫助評估該特徵的穩定性。較大的標準誤差意味著該特徵在不同腫瘤樣本中的變化較大。    \n",
    "  意義：標準誤差可以幫助衡量該特徵的穩定性或一致性。當某個特徵的標準誤差較小時，表示這個特徵在所有樣本中更為穩定，反之，標準誤差較大的特徵可能顯示出較大差異，這對於判斷某些腫瘤特徵的可靠性至關重要。對於模型預測，它幫助了解是否可以對某個特徵進行有效的預測。\n",
    "\n",
    "* worst：最大值或最惡劣情況的值（在該筆樣本的 10 個最大細胞中），揭示腫瘤的最極端情況，通常有助於識別惡性腫瘤的特徵。    \n",
    "  意義：最惡劣情況值能反映出腫瘤在極端情況下的特徵，這對於識別最具侵襲性的腫瘤（即惡性腫瘤）至關重要。惡性腫瘤通常會表現出最大的周長、最差的對稱性、最不平滑的邊界等，因此 worst 統計量是預測腫瘤是否惡性的重要指標。    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9841f867-b0cc-429d-b8bd-2026deb42edf",
   "metadata": {},
   "source": [
    "#### 綜合分析：結合所有三個統計量\n",
    "意義： 通過結合 mean、se 和 worst 進行多維度的觀察，可以更全面地了解腫瘤的特徵。\n",
    "* (1)惡性腫瘤：通常會顯示出mean與worst之間的顯著差距，se較高。這表明腫瘤在正常情況下表現較為穩定，但在極端情況下，特徵可能會劇烈變化，反映出其惡性。\n",
    "* (2)良性腫瘤：mean和worst之間的差異較小，且se較低，表明這些腫瘤的特徵在樣本間較為一致且較為規則。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ab5217-4b84-4971-b270-12d2a8207b92",
   "metadata": {},
   "source": [
    "### 標籤說明（target）\n",
    "0 = 惡性（malignant）    \n",
    "1 = 良性（benign）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5713e24e-1797-4e43-ad01-a455d0684bd2",
   "metadata": {},
   "source": [
    "# 2. 敘述性統計"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e2604c-6525-4e03-8f4f-a75924e02118",
   "metadata": {},
   "source": [
    "### 2.1 特徵分佈的直方圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb825db9-3fc6-435a-ae86-4e05065c232b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 設置畫布大小\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# 繪製每個特徵的直方圖\n",
    "df.drop('target', axis=1).hist(bins=30, figsize=(15, 10), layout=(6, 5))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b070b215-1244-45d0-8ca5-ec248eaec561",
   "metadata": {},
   "source": [
    "### 2.2 各特徵間的相關性熱圖"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c39192-25e6-4b07-b095-54b241e96da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 計算相關性矩陣\n",
    "correlation_matrix = df.drop('target', axis=1).corr()\n",
    "\n",
    "# 繪製相關性熱圖\n",
    "plt.figure(figsize=(20, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", fmt=\".2f\", linewidths=0.3)\n",
    "plt.title(\"Features Correlation Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442d654d-d223-4108-9d8d-068b9e31e9c1",
   "metadata": {},
   "source": [
    "### 2.3 不同類別的分佈（根據腫瘤類型）\n",
    "根據 target（腫瘤類型：良性或惡性）來觀察特徵的分佈，看看不同類型的腫瘤在這些特徵上有什麼區別。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c5213-7d10-44d3-b54d-cd772da34296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根據 target 來分組繪製箱型圖\n",
    "plt.figure(figsize=(20, 15))  # 增加畫布高度\n",
    "\n",
    "for i, feature in enumerate(df.columns[:-1]):  # 忽略 'target' 列\n",
    "    plt.subplot(6, 5, i + 1)  # 每行顯示 5 個子圖\n",
    "    sns.boxplot(x='target', y=feature, data=df)\n",
    "    plt.title(f\"Boxplot of {feature}\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31467141-e423-4a35-82b6-964759adb2b2",
   "metadata": {},
   "source": [
    "### 2.4 特徵與目標變數之間的關係\n",
    "用散點圖來檢視特徵與 target 變數（腫瘤類型）之間的關係。這有助於了解某些特徵是否對區分良性和惡性腫瘤有較大的幫助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd69504f-b4ec-4711-8a33-13a42fb7a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 繪製每個特徵與 target 的散點圖\n",
    "plt.figure(figsize=(15, 15))  # 增加畫布高度\n",
    "\n",
    "for i, feature in enumerate(df.columns[:-1]):  # 忽略 'target' 列\n",
    "    plt.subplot(6, 5, i + 1)  \n",
    "    sns.scatterplot(x=df[feature], y=df['target'], alpha=0.5)\n",
    "    plt.title(f\"{feature} vs Target\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2bf47c4-3b44-45bf-871c-836b96e1b95c",
   "metadata": {},
   "source": [
    "# 3. 建模"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e330309-e492-41f6-8f9c-de76b067c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割資料集為訓練集和測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    breast_cancer.data, breast_cancer.target, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce548dc-d34e-445b-86e4-09e8f7908ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義基本模型\n",
    "base_models = [\n",
    "    RandomForestClassifier(n_estimators=100, random_state=0),\n",
    "    XGBClassifier(n_estimators=100, random_state=0),\n",
    "    SVC(probability=True, random_state=0)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae40a04e-3802-4159-ab34-b4d9d87e329f",
   "metadata": {},
   "source": [
    "定義三個基本的機器學習模型，這些模型將用於訓練並生成最終預測結果的特徵（基模型的預測）。    \n",
    "這些基模型將在訓練過程中相互合作，然後它們的預測結果將作為特徵輸入到 meta-model（元模型）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b3881d-9ada-4b9e-8af4-63519db75c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義 meta-model\n",
    "meta_model = LogisticRegression(random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31e66fd-5c91-4786-8a03-c4f4b71f2902",
   "metadata": {},
   "source": [
    "meta-model 會基於基模型的預測結果進行學習，並產生最終的預測結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579bc53c-48a1-4557-895c-8775a9a3dc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定義交叉驗證的 fold 數量\n",
    "n_folds = 5   # 表示進行 5 折交叉驗證。即資料會被劃分成 5 個子集，每次用 4 個子集來訓練，剩下的 1 個子集用來驗證。\n",
    "\n",
    "# 初始化 arrays，用來保存基本模型和 meta-model 的訓練和測試預測結果\n",
    "base_model_train_pred = np.zeros((X_train.shape[0], len(base_models)))\n",
    "base_model_test_pred = np.zeros((X_test.shape[0], len(base_models)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69d11c3-f6d9-49dd-9e8f-da8d6e7e2916",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0)\n",
    "# StratifiedKFold: 是一種「保留類別比例」的 K 折交叉驗證法。\n",
    "# 比起一般 KFold，它會確保：每一折（fold）裡，目標變數（例如 churn = 1 / 0）的比例和整體資料集差不多。\n",
    "# 總結：StratifiedKFold 是讓交叉驗證在分類任務中更穩定、公平的做法，尤其適合處理類別不平衡的資料集。\n",
    "\n",
    "for i, model in enumerate(base_models):\n",
    "    for train_idx, valid_idx in kf.split(X_train, y_train):\n",
    "        X_train_fold, y_train_fold = X_train[train_idx], y_train[train_idx]\n",
    "        X_valid_fold, y_valid_fold = X_train[valid_idx], y_train[valid_idx]\n",
    "\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "        base_model_train_pred[valid_idx, i] = model.predict_proba(X_valid_fold)[:, 1]\n",
    "        base_model_test_pred[:, i] += model.predict_proba(X_test)[:, 1] / n_folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f6bb7-8b0f-457c-9115-3e3472ddb47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用基本模型的預測結果訓練 meta-model\n",
    "meta_model.fit(base_model_train_pred, y_train)\n",
    "\n",
    "# 使用 meta-model 預測測試集\n",
    "pred = meta_model.predict_proba(base_model_test_pred)[:, 1] # 對應於類別 \"1\" 的預測機率（通常是指正類，1 類）。這樣可以獲得每個樣本屬於類別 1 的預測機率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7dbdd7-e690-4124-a7f4-599c66f2661e",
   "metadata": {},
   "source": [
    "使用基本模型（base models）進行預測後，將其預測結果作為特徵來訓練一個 meta-model，並利用這個 meta-model 來進行最終的預測。\n",
    "最後，計算 log loss 來評估模型的預測性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8428f21-0ab3-463e-8f9d-a67cf690fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, log_loss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming the necessary variables (e.g., base_models, meta_model, etc.) are already defined\n",
    "\n",
    "# Define lists to store the metrics\n",
    "base_model_metrics = {\n",
    "    \"model\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1_score\": [],\n",
    "    \"auc_roc\": [],\n",
    "    \"log_loss\": [],\n",
    "    \"confusion_matrix\": []\n",
    "}\n",
    "\n",
    "meta_model_metrics = {\n",
    "    \"model\": [],\n",
    "    \"accuracy\": [],\n",
    "    \"precision\": [],\n",
    "    \"recall\": [],\n",
    "    \"f1_score\": [],\n",
    "    \"auc_roc\": [],\n",
    "    \"log_loss\": [],\n",
    "    \"confusion_matrix\": []\n",
    "}\n",
    "\n",
    "# Evaluate the base models\n",
    "for i, model in enumerate(base_models):\n",
    "    print(f\"Evaluating model {i+1}: {model.__class__.__name__}\")\n",
    "    \n",
    "    # Training set evaluation\n",
    "    y_train_pred = base_model_train_pred[:, i]\n",
    "    \n",
    "    # For classification metrics, convert probabilities to class predictions (0 or 1)\n",
    "    y_train_pred_class = (y_train_pred >= 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_train, y_train_pred_class)\n",
    "    precision = precision_score(y_train, y_train_pred_class)\n",
    "    recall = recall_score(y_train, y_train_pred_class)\n",
    "    f1 = f1_score(y_train, y_train_pred_class)\n",
    "    auc_roc = roc_auc_score(y_train, y_train_pred)\n",
    "    logloss = log_loss(y_train, y_train_pred)\n",
    "    conf_matrix = confusion_matrix(y_train, y_train_pred_class)\n",
    "    \n",
    "    # Store metrics\n",
    "    base_model_metrics[\"model\"].append(model.__class__.__name__)\n",
    "    base_model_metrics[\"accuracy\"].append(accuracy)\n",
    "    base_model_metrics[\"precision\"].append(precision)\n",
    "    base_model_metrics[\"recall\"].append(recall)\n",
    "    base_model_metrics[\"f1_score\"].append(f1)\n",
    "    base_model_metrics[\"auc_roc\"].append(auc_roc)\n",
    "    base_model_metrics[\"log_loss\"].append(logloss)\n",
    "    base_model_metrics[\"confusion_matrix\"].append(conf_matrix)\n",
    "    \n",
    "    # Test set evaluation\n",
    "    y_test_pred = base_model_test_pred[:, i]\n",
    "    \n",
    "    # Convert probabilities to class predictions for test set\n",
    "    y_test_pred_class = (y_test_pred >= 0.5).astype(int)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_test_pred_class)\n",
    "    precision = precision_score(y_test, y_test_pred_class)\n",
    "    recall = recall_score(y_test, y_test_pred_class)\n",
    "    f1 = f1_score(y_test, y_test_pred_class)\n",
    "    auc_roc = roc_auc_score(y_test, y_test_pred)\n",
    "    logloss = log_loss(y_test, y_test_pred)\n",
    "    conf_matrix = confusion_matrix(y_test, y_test_pred_class)\n",
    "    \n",
    "    # Store metrics for test set\n",
    "    base_model_metrics[\"model\"].append(model.__class__.__name__ + \"_test\")\n",
    "    base_model_metrics[\"accuracy\"].append(accuracy)\n",
    "    base_model_metrics[\"precision\"].append(precision)\n",
    "    base_model_metrics[\"recall\"].append(recall)\n",
    "    base_model_metrics[\"f1_score\"].append(f1)\n",
    "    base_model_metrics[\"auc_roc\"].append(auc_roc)\n",
    "    base_model_metrics[\"log_loss\"].append(logloss)\n",
    "    base_model_metrics[\"confusion_matrix\"].append(conf_matrix)\n",
    "\n",
    "# Now evaluate the meta-model (Logistic Regression)\n",
    "print(f\"Evaluating meta-model: {meta_model.__class__.__name__}\")\n",
    "\n",
    "# Training set evaluation for meta-model\n",
    "y_train_pred_meta = meta_model.predict_proba(base_model_train_pred)[:, 1]\n",
    "\n",
    "# Convert probabilities to class predictions for the meta-model\n",
    "y_train_pred_meta_class = (y_train_pred_meta >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_train, y_train_pred_meta_class)\n",
    "precision = precision_score(y_train, y_train_pred_meta_class)\n",
    "recall = recall_score(y_train, y_train_pred_meta_class)\n",
    "f1 = f1_score(y_train, y_train_pred_meta_class)\n",
    "auc_roc = roc_auc_score(y_train, y_train_pred_meta)\n",
    "logloss = log_loss(y_train, y_train_pred_meta)\n",
    "conf_matrix = confusion_matrix(y_train, y_train_pred_meta_class)\n",
    "\n",
    "# Store metrics for meta-model\n",
    "meta_model_metrics[\"model\"].append(\"meta_model\")\n",
    "meta_model_metrics[\"accuracy\"].append(accuracy)\n",
    "meta_model_metrics[\"precision\"].append(precision)\n",
    "meta_model_metrics[\"recall\"].append(recall)\n",
    "meta_model_metrics[\"f1_score\"].append(f1)\n",
    "meta_model_metrics[\"auc_roc\"].append(auc_roc)\n",
    "meta_model_metrics[\"log_loss\"].append(logloss)\n",
    "meta_model_metrics[\"confusion_matrix\"].append(conf_matrix)\n",
    "\n",
    "# Test set evaluation for meta-model\n",
    "y_test_pred_meta = meta_model.predict_proba(base_model_test_pred)[:, 1]\n",
    "\n",
    "# Convert probabilities to class predictions for meta-model test set\n",
    "y_test_pred_meta_class = (y_test_pred_meta >= 0.5).astype(int)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_test_pred_meta_class)\n",
    "precision = precision_score(y_test, y_test_pred_meta_class)\n",
    "recall = recall_score(y_test, y_test_pred_meta_class)\n",
    "f1 = f1_score(y_test, y_test_pred_meta_class)\n",
    "auc_roc = roc_auc_score(y_test, y_test_pred_meta)\n",
    "logloss = log_loss(y_test, y_test_pred_meta)\n",
    "conf_matrix = confusion_matrix(y_test, y_test_pred_meta_class)\n",
    "\n",
    "# Store metrics for meta-model test set\n",
    "meta_model_metrics[\"model\"].append(\"meta_model_test\")\n",
    "meta_model_metrics[\"accuracy\"].append(accuracy)\n",
    "meta_model_metrics[\"precision\"].append(precision)\n",
    "meta_model_metrics[\"recall\"].append(recall)\n",
    "meta_model_metrics[\"f1_score\"].append(f1)\n",
    "meta_model_metrics[\"auc_roc\"].append(auc_roc)\n",
    "meta_model_metrics[\"log_loss\"].append(logloss)\n",
    "meta_model_metrics[\"confusion_matrix\"].append(conf_matrix)\n",
    "\n",
    "# Convert metrics dictionaries to DataFrames\n",
    "base_model_metrics_df = pd.DataFrame(base_model_metrics)\n",
    "meta_model_metrics_df = pd.DataFrame(meta_model_metrics)\n",
    "\n",
    "# Combine base models and meta-model metrics\n",
    "all_metrics_df = pd.concat([base_model_metrics_df, meta_model_metrics_df], ignore_index=True)\n",
    "\n",
    "# Display the final metrics DataFrame\n",
    "print(\"\\n--- Final Model Evaluation Metrics ---\")\n",
    "all_metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f1f614-c221-4a42-82b6-b73fe7b536ed",
   "metadata": {},
   "source": [
    "* Log loss 越小越好：log loss 衡量的是預測概率與真實標籤之間的差距。理論上，log loss 的範圍是從 0 開始（完全準確的預測）到正無窮大（極差的預測）。因此，數值越小表示模型的預測越準確。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef9a75c-6b1b-417e-9305-1acab82a8172",
   "metadata": {},
   "source": [
    "# 新的資料如何使用 stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e721f299-738b-45bd-886b-6c320992f244",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 假設 new_data 的 shape 為 (10, 30)，即有 10 筆資料，每筆資料有 30 個特徵\n",
    "new_data = np.random.rand(10, 30)\n",
    "\n",
    "# 使用已訓練好的 base models 預測 new_data\n",
    "base_model_pred = np.zeros((10, len(base_models)))  # 因為有 10 筆資料，所以維度是 (10, len(base_models))\n",
    "\n",
    "for i, model in enumerate(base_models):\n",
    "    base_model_pred[:, i] = model.predict_proba(new_data)[:, 1]  # 預測每個模型的正樣本機率\n",
    "\n",
    "# 使用 meta-model 進行預測\n",
    "meta_model_pred = meta_model.predict_proba(base_model_pred)[:, 1]  # 這裡的 shape 是 (10,)，代表 10 筆資料的預測結果\n",
    "\n",
    "meta_model_pred  # 輸出每筆資料的預測結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147bad29-ef16-41f2-ad1e-8b82b10294d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_predictions = (meta_model_pred > 0.5).astype(int)\n",
    "print(final_predictions)  # 顯示每筆資料的最終分類結果 (0 或 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51149e09-5853-47b3-ae10-4a3a2487197f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ispan-machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
